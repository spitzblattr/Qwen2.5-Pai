#!/bin/bash
set -e  
ENV=$1
export PYTHONWARNINGS="ignore::FutureWarning"
export SSL_CERT_FILE=/home/cacert.pem
export CUDA_DEVICE_MAX_CONNECTIONS=1
export NVTE_FLASH_ATTN=1
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
#export UB_SKIPMC=1     # â† å¦‚æœæœ‰AssertionError: CUDA device, driver and/or toolkit version does not support comm+GEMM overlap with CUDA Multicast. Launch app with UB_SKIPMC=1 to try CUDA IPC instead
export MCCL_MAX_NCHANNELS=16
export PYTHONPATH=/workspace/Pai-Megatron-Patch:/workspace/Pai-Megatron-Patch/Megatron-LM:$PYTHONPATH  

# --------------------------------------------------------
IS_INITIAL_RUN=true    # ğŸš© # æ˜¯å¦ä¸ºé¦–æ¬¡è®­ç»ƒ
# ğŸ’¡æ²æ›¦ä¸Šç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æ£€æŸ¥çš„å§‹ç»ˆæ˜¯torchæ ¼å¼è€Œä¸æ˜¯torch-distï¼Œæ‰€ä»¥è®­ç»ƒå®Œæ¯•æ—¶ä¸éœ€è¦å†æ¬¡è¿è¡Œè¿™ä¸ªè„šæœ¬ä»torch-distè½¬å›torchï¼Œç›´æ¥å¯ä»¥è¿è¡Œ[2]convert_between_hf_megatron.sh
# --------------------------------------------------------
DATASET_PATH=/mnt/public/data/spitzblattr/demo/demo_text_document  # è¿™ä¸ªè·¯å¾„åé¢ä¼šç›´æ¥è¿½åŠ ".bin"/"idx"
LOAD_CHECKPOINT_PATH=/mnt/public/model/spitzblattr/training_models/Qwen2.5-7B-Instruct                    
# â†‘â†‘ LOAD_CHECKPOINT_PATH ä¸‹å¿…é¡»è¦æœ‰config.jsonã€vocab.jsonã€tokenizer.jsonç­‰ï¼Œå¹¶ä¸”è¿˜è¦æœ‰ä¸€ä¸ªlatest_checkpointed_iteration.txt
SAVE_TORCH_CHECKPOINT_PATH=/mnt/public/model/spitzblattr/training_models/saved_ckpts 
WANDB_SAVE_PATH=/home/wandb_logs   # Path to save the wandb results locally.
# --------------------------------------------------------
# â­ï¸ â­ï¸ â­ï¸ï¼šè¯¥sectionä¸‹æ‰€æœ‰å‚æ•°éƒ½å¿…é¡»æ ¡éªŒ
# â­ï¸ï¼šè¯¥å•ä¸ªå‚æ•°å¿…é¡»æ ¡éªŒï¼ˆå¦‚æœæ²¡æœ‰è¿™ä¸ªæ ‡è®°ï¼Œå¹¶ä¸æ„å‘³ç€è¿™ä¸ªå‚æ•°ä¸€å®šä¸éœ€è¦æ ¡éªŒï¼Œåªæ˜¯å®ƒ*æˆ–è®¸å¯ä»¥*ä¸æ ¡éªŒï¼‰
# ğŸŒ™ï¼šè¿™ä¸ªå‚æ•°æ˜¯ PAI æ–°å¢çš„ï¼Œä¸æ˜¯åŸºç¡€ Megatron é‡Œçš„
# æ‰€æœ‰å‚æ•°è§ Pai-Megatron-Patch/Megatron-LM/megatron/training/arguments.py

DATA_ARGS=(
    --data-path $DATASET_PATH  # å…³äºæ··åˆæ¯”ä¾‹æ•°æ®é›†(blended dataset)çš„è§£é‡Šï¼šhttps://github.com/NVIDIA/Megatron-LM/issues/547
    --split 98,2,0 # â­ï¸ # è‹¥è¦æŒ‡å®šå•ç‹¬çš„validæ•°æ®é›†æ¥æºï¼Œä½¿ç”¨--valid-data-path
    --seq-length 2048   # â­ï¸ # Megatron-LM requires the input sequence length to be fixed and padded to the --seq-length
    --dataset MMAP  # ğŸŒ™  # "JSON-SFT"(æœªå¤„ç†çš„.jsonæ–‡ä»¶) or "MMAP"(å¤„ç†å®Œæ¯•çš„binå’Œidx)
)


# â­ï¸ â­ï¸ â­ï¸
DISTRIBUTED_ARGS=(
    --nproc_per_node 8
    --nnodes 1
    --node_rank 0 
    --master_addr localhost
    --master_port 22222
)

# â­ï¸ â­ï¸ â­ï¸
# 7B
GPT_MODEL_CONFIGJSON_ARGS=(
    --num-layers 28
    --hidden-size 3584 
    --num-attention-heads 28
    --ffn-hidden-size 18944   
    --max-position-embeddings 131072
    --group-query-attention 
    --num-query-groups 4      
    --normalization RMSNorm 
    --norm-epsilon 1e-6    
    --extra-vocab-size 421  
    --untie-embeddings-and-output-weights 
    # --use-cpu-initialization 
)
# â­ï¸ â­ï¸ â­ï¸
GPT_MODEL_ADDITIONAL_ARGS=(
    --swiglu  
    --position-embedding-type rope  
    --disable-bias-linear # å…ˆå»é™¤æ‰€æœ‰çº¿æ€§å±‚çš„ bias
    --add-qkv-bias   # å†åœ¨ q,k,v_proj å±‚æ·»åŠ  bias
    --rotary-base 1000000    # Base to use for rotary positional embeddings. é»˜è®¤ä¸º10000
    --max-position-embeddings 32768    # qwen2.5
    --rotary-percent 1.0   
    --rotary-seq-len-interpolation-factor 1 
    --patch-tokenizer-type Qwen2Tokenizer # ğŸŒ™ 
)


TRAINING_ARGS=(
    --bf16  # â­ï¸  
    --micro-batch-size 2 # â­ï¸ 
    --global-batch-size 16 # â­ï¸ # should be a multiple of micro-batch-size times data-parallel-sizeã€‚å½“æ˜¾å¼æŒ‡å®šäº† --global-batch-size æ—¶ï¼ŒMegatron ä¼šæ ¹æ®æ­¤å€¼è‡ªåŠ¨æ¨æ–­ num_micro_batches çš„å¤§å°
    --weight-decay 0.01  
    --init-method-std 0.02
    --clip-grad 1.0 
    --lr 6.5e-5       # â­ï¸
    --min-lr 2.5e-6   # â­ï¸
    --lr-warmup-iters 0
    --lr-decay-style cosine 
    #--lr-decay-iters: If None defaults to `--train-iters`
    --attention-dropout 0.0     # é»˜è®¤å¯ç”¨å¹¶ä¸º0.1
    --hidden-dropout 0.0    # é»˜è®¤å¯ç”¨å¹¶ä¸º0.1
    # --calculate-per-token-loss    # æ²æ›¦ç‰ˆMegatronæœªå®ç°
    --train-mode finetune  # â­ï¸ğŸŒ™  # type=str, help="pretrain or finetune"ã€‚*å¿…é¡»æŒ‡å®šè¯¥å‚æ•°* # è¯¥å‚æ•°å½±å“ megatron_patch/template/helper.py ä¸­çš„ get_batch å‡½æ•°
    --transformer-impl local  # æ²æ›¦2.27ä¸æ”¯æŒ transformer_engine
)
num_epoches=3   # â­ï¸
num_training_samples=154678    # â­ï¸ # è‡ªè¡ŒæŸ¥çœ‹
train_iters=$(( num_epoches * num_training_samples / ${TRAINING_ARGS[4]} ))    # TRAINING_ARGS[4]æ˜¯--global-batch-size 
TRAINING_ARGS+=(
    --train-iters $train_iters   
)

MODEL_PARALLEL_ARGS=(
	--tensor-model-parallel-size 2   # â­ï¸
	--pipeline-model-parallel-size 2  # â­ï¸
    --recompute-activations  # åœ¨å¤šæ•°æƒ…å†µä¸‹éƒ½åº”è¯¥å¯ç”¨ï¼Œé™¤éè®­ç»ƒå†…å­˜éå¸¸æœ‰é™
    --use-distributed-optimizer   # æ˜¯å¦ä½¿ç”¨Megatronç‰ˆZero-1ä¼˜åŒ–å™¨: , action='store_true'
    #--optimizer hybridadam    # ğŸŒ™ # æ²æ›¦2.27æœªå®ç° # ç›¸å…³ä»£ç ï¼šhttps://github.com/alibaba/Pai-Megatron-Patch/pull/298/files (åªå­˜åœ¨äºPAI-Megatron-LM-240718åˆ†æ”¯)
    #--optimizer-offload-policy auto     # ğŸŒ™ # æ²æ›¦2.27æœªå®ç°
)
# if --tensor-model-parallel-size > 1
if [ ${MODEL_PARALLEL_ARGS[1]} -gt 1 ]; then
    MODEL_PARALLEL_ARGS+=(
        --sequence-parallel  # When --sequence-parallel is used, sequence_len must be a multiple of --tensor-parallel. 
        --context-parallel-size 1    # Degree of context parallelism. é»˜è®¤ä¸º1.
        # ä»¥ä¸‹ æ²æ›¦2.27æœªå®ç°ï¼Œéœ€è¦ transformer engine å’Œ mcore æ ¼å¼æ¨¡å‹
        #--tp-comm-overlap  
        #--overlap-grad-reduce
        #--overlap-param-gather   
    )
fi
# if --pipeline-model-parallel-size > 1
if [ ${MODEL_PARALLEL_ARGS[3]} -gt 1 ]; then
    MODEL_PARALLEL_ARGS+=(
        # Enable p2p comm overlap when PP > 1 by setting num_layers_per_virtual_pipeline_stage.
        # --num-layers-per-virtual-pipeline-stage 1
        # æ²æ›¦æœªå®ç°
    )
fi


EVAL_LOGGING_AND_SAVE_ARGS=(
    --save-interval 100 # â­ï¸
    --eval-iters 50      # Number of iterations to run for evaluation validation/test for. default=100
    --eval-interval 100  # â­ï¸ # Interval between running evaluation on validation set. default=1000
    --save $SAVE_TORCH_CHECKPOINT_PATH 
    --load $LOAD_CHECKPOINT_PATH 
    --exit-on-missing-checkpoint    # å¦‚æœæ‰¾ä¸åˆ°--loadä¸­çš„è·¯å¾„ï¼Œç›´æ¥é€€å‡º
    --auto-detect-ckpt-format   # æ¯æ¬¡ï¼ˆé‡æ–°æˆ–ç»§ç»­ï¼‰è®­ç»ƒå‰ï¼Œè‡ªåŠ¨æ£€æŸ¥$LOAD_CHECKPOINT_PATH çš„ checkpoint æ˜¯åŸå§‹torchæ ¼å¼è¿˜æ˜¯distcpæ ¼å¼
    
    --log-interval 100   
    --log-throughput   # If set, calculate and log throughput per GPU.
)
if [ ${IS_INITIAL_RUN} = true ]; then
    EVAL_LOGGING_AND_SAVE_ARGS+=(
        --no-load-optim   # When start initial training, specify `--no-load-optim --finetune` to make sure the optimizer state is NOT loaded from the pretrained GPT model so the continued pretraining is from a clean start. But, after the first run, you should launch without`--no-load-optim --finetune` to make sure the optimizer state is correctly loaded from your last job.
        --no-load-rng 
    )
fi
EVAL_LOGGING_AND_SAVE_ARGS+=(
    #--async-save   # æ²æ›¦ Megatronæœªå®ç°
    # ä¸éœ€è¦æŒ‡å®š wandb çš„ team(shsfcx)ï¼Œè¿™ä¼šåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œè„šæœ¬æ—¶æ‰‹åŠ¨è®©ä½ è¾“å…¥
    --wandb-project spec_decode   # â­ï¸
    --wandb-exp-name qwen-sft  # â­ï¸ 
    --wandb-save-dir $WANDB_SAVE_PATH 
)



torchrun ${DISTRIBUTED_ARGS[@]} /workspace/Pai-Megatron-Patch/examples/qwen2_5/pretrain_qwen.py \
    ${DATA_ARGS[@]} \
    ${MODEL_PARALLEL_ARGS[@]} \
    ${GPT_MODEL_CONFIGJSON_ARGS[@]} \
    ${GPT_MODEL_ADDITIONAL_ARGS[@]} \
    ${TRAINING_ARGS[@]} \
    ${EVAL_LOGGING_AND_SAVE_ARGS[@]} \

set +x


<<'EOF'


EOF