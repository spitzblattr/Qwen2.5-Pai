#!/bin/bash
export PYTHONWARNINGS="ignore::FutureWarning"
export PYTHONPATH=/workspace/Pai-Megatron-Patch-0.10.3:/workspace/Pai-Megatron-Patch-0.10.3/Megatron-LM-core_r0.11.1:$PYTHONPATH   


# --------------------------------------------------------
# paths
CONVERT_TYPE=megatron-to-huggingface    # ğŸš©

# >>>>>>> ä»¥ä¸‹æ˜¯å½“ CONVERT_TYPE=huggingface-to-megatron æ—¶çš„è·¯å¾„è®¾ç½®ï¼Œå¦åˆ™æ³¨é‡Šæ‰ä»¥ä¸‹
# LOAD_CHECKPOINT_PATH=/mnt/public/models/Qwen2.5-14B-Instruct      # åŸå§‹ huggingfaceæ¨¡å‹è·¯å¾„
# SAVE_CHECKPOINT_PATH=/mnt/public/training_models/Qwen2.5-14B-Instruct  # è¿™ä¸ªè·¯å¾„ä¸‹å¿…é¡»æ˜¯ç©ºç™½çš„

# >>>>>>> ä»¥ä¸‹æ˜¯å½“ CONVERT_TYPE=megatron-to-huggingface æ—¶çš„è·¯å¾„è®¾ç½®ï¼Œå¦åˆ™æ³¨é‡Šæ‰ä»¥ä¸‹
HF_MODEL_PATH=/mnt/public/models/Qwen2.5-14B-Instruct  # åŸå§‹ huggingfaceæ¨¡å‹è·¯å¾„
LOAD_CHECKPOINT_PATH=/mnt/public/train_finished_models  # è®­ç»ƒå®Œæˆçš„æ¨¡å‹
SAVE_CHECKPOINT_PATH=/mnt/public/train_finished_models/final   # æ³¨è¿™ä¸ªè·¯å¾„ä¸‹å¿…é¡»æ˜¯ç©ºç™½çš„
# --------------------------------------------------------

echo "CONVERT_TYPE: $CONVERT_TYPE " 
if [ $CONVERT_TYPE = "megatron-to-huggingface" ]; then
    convert_args=(
        --convert-checkpoint-from-megatron-to-transformers
        --hf-ckpt-path ${HF_MODEL_PATH}
    )
elif [ $CONVERT_TYPE = "huggingface-to-megatron" ]; then
    convert_args=()
else
    echo "[ERROR] CONVERT_TYPE must be \"huggingface-to-megatron\" or \"megatron-to-huggingface\"." 
    exit 1
fi


distributed_args=(
    --nproc_per_node 1
    --nnodes 1
    --node_rank 0
    --master_addr localhost
    --master_port 21111
)

# æ‰€æœ‰å‚æ•°é‡çš„model_config_argsè§æ–‡ä»¶ä¸‹æ–¹
model_config_args=(
    --num-layers 48
    --hidden-size 5120 
    --num-attention-heads 40
    --ffn-hidden-size 13824   
    --max-position-embeddings 131072
    --group-query-attention 
    --num-query-groups 8      
    --normalization RMSNorm 
    --norm-epsilon 1e-5    
    --extra-vocab-size 421 
    --untie-embeddings-and-output-weights 
    # --use-cpu-initialization 
)

additional_args=(
    --load $LOAD_CHECKPOINT_PATH
    --save $SAVE_CHECKPOINT_PATH
    --target-tensor-model-parallel-size 2
    --target-pipeline-model-parallel-size 2
    #--target-num-layers-per-virtual-pipeline-stage 1
    --micro-batch-size 1  # éšä¾¿å†™
    --save-interval 1     # éšä¾¿å†™
    # --------------------------
    # ä»¥ä¸‹å‚æ•°éœ€è¦å’Œè®­ç»ƒæ—¶ï¼ˆ[3]run_mcore_qwen.sh ä¸­çš„ GPT_MODEL_ADDITIONAL_ARGSï¼‰ç›¸åŒ
    --swiglu
    --seq-length 2048
    --no-async-tensor-model-parallel-allreduce
    --patch-tokenizer-type Qwen2Tokenizer
    --no-bias-swiglu-fusion
    --no-rope-fusion
    --use-rotary-position-embeddings
    --disable-bias-linear
    --add-qkv-bias
    --attention-dropout 0.0
    --hidden-dropout 0.0
    --rotary-base 1000000
    # --------------------------
    --use-mcore-models
    --save-safetensors
    --bf16
    --transformer-impl transformer_engine  # è½¬æ¢ä¸º mcore æ ¼å¼æ¨¡å‹
)


torchrun ${distributed_args[@]} /workspace/Pai-Megatron-Patch-0.10.3/toolkits/model_checkpoints_convertor/qwen/hf2mcore_qwen2_dense_and_moe_gqa.py \
    ${model_config_args[@]} \
    ${additional_args[@]} \
    ${convert_args[@]} \

set +x

<<'EOF'



ã€é™„: qwen ç³»åˆ— configã€‘

if [ $MODEL_SIZE = 0.5B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 24
        --hidden-size 896 
        --num-attention-heads 14
        --ffn-hidden-size 4864    # æ¨¡å‹config.jsoné‡Œçš„INTERMEDIATE_SIZE
        --max-position-embeddings 32768
        --group-query-attention 
        --num-query-groups 2       # æ¨¡å‹config.jsoné‡Œçš„NUM_KEY_VALUE_HEADS
        --normalization RMSNorm 
        --norm-epsilon 1e-6    # æ¨¡å‹config.jsoné‡Œçš„RMS_NORM_EPS
        --extra-vocab-size 293  # æ¨¡å‹config.jsoné‡Œçš„EXTRA_VOCAB_SIZE
        # --untie-embeddings-and-output-weights # qwen2.5ç³»åˆ—ï¼šå¦‚æœå‚æ•°é‡>=7B,åˆ™å¯ç”¨--untie-embeddings-and-output-weightsï¼Œå¦åˆ™ä¸å¯ç”¨
        # --use-cpu-initialization  # åªåœ¨72Bæ¨¡å‹æ—¶å¯ç”¨
    )

elif [ $MODEL_SIZE = 1.5B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 28
        --hidden-size 1536 
        --num-attention-heads 12
        --ffn-hidden-size 8960   
        --max-position-embeddings 32768
        --group-query-attention 
        --num-query-groups 2      
        --normalization RMSNorm 
        --norm-epsilon 1e-6   
        --extra-vocab-size 293 
        # --untie-embeddings-and-output-weights
        # --use-cpu-initialization 
    )

elif [ $MODEL_SIZE = 3B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 36
        --hidden-size 2048 
        --num-attention-heads 16
        --ffn-hidden-size 11008   
        --max-position-embeddings 32768
        --group-query-attention 
        --num-query-groups 2       
        --normalization RMSNorm 
        --norm-epsilon 1e-6   
        --extra-vocab-size 293
        # --untie-embeddings-and-output-weights 
        # --use-cpu-initialization  
    )

elif [ $MODEL_SIZE = 7B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 28
        --hidden-size 3584 
        --num-attention-heads 28
        --ffn-hidden-size 18944   
        --max-position-embeddings 131072
        --group-query-attention 
        --num-query-groups 4      
        --normalization RMSNorm 
        --norm-epsilon 1e-6    
        --extra-vocab-size 421  
        --untie-embeddings-and-output-weights 
        # --use-cpu-initialization 
    )

elif [ $MODEL_SIZE = 14B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 48
        --hidden-size 5120 
        --num-attention-heads 40
        --ffn-hidden-size 13824   
        --max-position-embeddings 131072
        --group-query-attention 
        --num-query-groups 8      
        --normalization RMSNorm 
        --norm-epsilon 1e-5    
        --extra-vocab-size 421 
        --untie-embeddings-and-output-weights 
        # --use-cpu-initialization 
    )

elif [ $MODEL_SIZE = 32B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 64
        --hidden-size 5120 
        --num-attention-heads 40
        --ffn-hidden-size 27648   
        --max-position-embeddings 131072
        --group-query-attention 
        --num-query-groups 8      
        --normalization RMSNorm 
        --norm-epsilon 1e-5   
        --extra-vocab-size 421 
        --untie-embeddings-and-output-weights 
        # --use-cpu-initialization  
    )

elif [ $MODEL_SIZE = 72B ]; then
    GPT_MODEL_CONFIGJSON_ARGS=(
        --num-layers 80
        --hidden-size 8192 
        --num-attention-heads 64 
        --ffn-hidden-size 29568   
        --max-position-embeddings 131072
        --group-query-attention 
        --num-query-groups 8       
        --normalization RMSNorm 
        --norm-epsilon 1e-5   
        --extra-vocab-size 421 
        --untie-embeddings-and-output-weights 
        --use-cpu-initialization  
    )
fi

EOF